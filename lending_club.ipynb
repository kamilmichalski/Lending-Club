{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task I will be using jupyter notebook since it's the best way to showcase my reasoning. I'll try to explain my exact thought process as I would go along with the code. I will be working on n=50000 sample of data due to lack of computer power. \n",
    "\n",
    "Let's start by loading data and looking through it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font size=\"10\"> Preprocessing </font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"Documents/lending-club-loan-data/loan.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_sample_data(data_path, num_samples=50000):\n",
    "    \n",
    "    df = pd.read_csv(data_path, low_memory=False)\n",
    "    df = df.sample(num_samples)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns:  74 \n",
      "Column names:  ['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose', 'title', 'zip_code', 'addr_state', 'dti', 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d', 'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m']\n"
     ]
    }
   ],
   "source": [
    "df = load_and_sample_data(DATA_PATH)\n",
    "print('Number of columns: ', len(df.columns), '\\nColumn names: ', df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Let's categorize loans into two categories and see how many examples fall into each one of them. I'll assign 'bad loans' as ones and 'good loans' as zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.93306\n",
       "1.0    0.06694\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_interest = ['Charged Off', \n",
    "               'Does not meet the credit policy. Status:Charged Off', \n",
    "               'Late (31-120 days)', \n",
    "               'Default']\n",
    "\n",
    "df['labels'] = np.where(df['loan_status'].isin(in_interest), 1.0, 0.0)\n",
    "df['labels'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Essentialy we're now facing binary classification problem with seriously unbalanced data. We may have to deal with this disproportion later on due to overfitting by applying regularization or data augmentation techniques, but let's first see how our models deal with data as they are.\n",
    "\n",
    "First we need to clean our data and deal with NaN values. Let's see how many NaNs each of our features contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annual_inc_joint               0.99954\n",
       "verification_status_joint      0.99954\n",
       "dti_joint                      0.99954\n",
       "il_util                        0.97954\n",
       "mths_since_rcnt_il             0.97718\n",
       "inq_fi                         0.97652\n",
       "inq_last_12m                   0.97652\n",
       "all_util                       0.97652\n",
       "max_bal_bc                     0.97652\n",
       "open_rv_24m                    0.97652\n",
       "total_cu_tl                    0.97652\n",
       "total_bal_il                   0.97652\n",
       "open_il_24m                    0.97652\n",
       "open_il_12m                    0.97652\n",
       "open_il_6m                     0.97652\n",
       "open_acc_6m                    0.97652\n",
       "open_rv_12m                    0.97652\n",
       "desc                           0.85326\n",
       "mths_since_last_record         0.84592\n",
       "mths_since_last_major_derog    0.75070\n",
       "mths_since_last_delinq         0.51432\n",
       "next_pymnt_d                   0.28720\n",
       "tot_cur_bal                    0.08044\n",
       "tot_coll_amt                   0.08044\n",
       "total_rev_hi_lim               0.08044\n",
       "emp_title                      0.05714\n",
       "emp_length                     0.05014\n",
       "last_pymnt_d                   0.01910\n",
       "revol_util                     0.00068\n",
       "title                          0.00030\n",
       "                                ...   \n",
       "home_ownership                 0.00000\n",
       "sub_grade                      0.00000\n",
       "grade                          0.00000\n",
       "installment                    0.00000\n",
       "int_rate                       0.00000\n",
       "term                           0.00000\n",
       "funded_amnt_inv                0.00000\n",
       "url                            0.00000\n",
       "loan_amnt                      0.00000\n",
       "member_id                      0.00000\n",
       "funded_amnt                    0.00000\n",
       "out_prncp_inv                  0.00000\n",
       "purpose                        0.00000\n",
       "zip_code                       0.00000\n",
       "application_type               0.00000\n",
       "policy_code                    0.00000\n",
       "last_pymnt_amnt                0.00000\n",
       "collection_recovery_fee        0.00000\n",
       "recoveries                     0.00000\n",
       "total_rec_late_fee             0.00000\n",
       "total_rec_int                  0.00000\n",
       "total_rec_prncp                0.00000\n",
       "total_pymnt_inv                0.00000\n",
       "total_pymnt                    0.00000\n",
       "out_prncp                      0.00000\n",
       "initial_list_status            0.00000\n",
       "revol_bal                      0.00000\n",
       "dti                            0.00000\n",
       "addr_state                     0.00000\n",
       "id                             0.00000\n",
       "Length: 75, dtype: float64"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance some of these features are completely useless for us. There seem to be big leap between <b>next_pymnt_d</b> and <b>total_rev_hi_lim</b>, so we may choose 0.1 threshold, noting that this decision is arbitrary and we may revisit this later on.\n",
    "\n",
    "Let's drop all columns that exceed our threshold and check with how many we're left: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped columns:  22\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1  \n",
    "\n",
    "mask = df.isnull().sum()/len(df) <= threshold\n",
    "print('Number of dropped columns: ', mask.value_counts()[False])\n",
    "\n",
    "df = df[df.columns[mask]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now eliminate every left feature that won't provide any useful information given our task. I'm gonna approach this very cautiously due to my lack of expertise on the subject of loans and delete only obviously redundant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns = ['loan_status', 'id', 'zip_code', 'policy_code', 'title', 'url', 'grade']\n",
    "df = df.drop(redundant_columns, axis=1)\n",
    "# I've dropped 'grade' category since 'sub_grade' contain all that information already. The rest, I think, is self-explanatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still left with 47 features. Some of them are categorical, so first let's find out which ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "term                       2\n",
       "sub_grade                 35\n",
       "emp_title              26311\n",
       "emp_length                11\n",
       "home_ownership             5\n",
       "verification_status        3\n",
       "issue_d                  102\n",
       "pymnt_plan                 2\n",
       "purpose                   14\n",
       "addr_state                49\n",
       "earliest_cr_line         595\n",
       "initial_list_status        2\n",
       "last_pymnt_d              94\n",
       "last_credit_pull_d        89\n",
       "application_type           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Upon further inspection (which I won't be presenting here since it would take too much space) I found out that:\n",
    "\n",
    "<b>pymnt_plan</b> and <b>application_type</b> can be deleted, because the first one has only one value and the second one has 9996:4 disproportion between labels. Also <b>member_id</b> can be dropped since there are no members who took more than one loan.\n",
    "\n",
    "<b>issue_d, earliest_cr_line, last_pymnt_d, last_credit_pull_d </b>are dates. They have too many unique values to represent them as one-hot vectors (too big dimensionality for my computer). I might put them in fewer time period brackets (eg. by years), but we would lose a lot of information that way. I decided to use following heuristic: encode dates as unix-time format. It can be beneficial in case I will be using any distance-based methods further down the line. \n",
    "\n",
    "<b>addr_state, sub_grade, emp_length, home_ownership, verification_status</b> and purpose all have less than 50 unique values, I can encode them as one-hots or process with sclearn LabelEncoder.\n",
    "\n",
    "<b>emp_title</b> has 6413 unique values, but only 141 of them appear more than 5 times, and only 50 more than 10. I will use the latter as cut-off and put all titles which appeared less than 50 times in 'Other' category. If during evaluation of feature importance I will find out that employment title is important, I will investigate this category further. \n",
    "\n",
    "So, here's the code for discrabed steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE PYMNT_PLAN, APPLICATION_TYPE AND MEMBER ID\n",
    "\n",
    "df = df.drop(columns=['pymnt_plan','application_type', 'member_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATES\n",
    "\n",
    "def _convert_string_to_unix_timestamp(date):\n",
    "    dt = datetime.strptime(date, '%b-%Y')\n",
    "    epoch = datetime(1970, 1, 1) \n",
    "    diff = dt - epoch\n",
    "    # I need to use this workaround since mktime doesn't support dates from before the Unix epoch (1970) on Windows\n",
    "    try:\n",
    "        timestamp = time.mktime(dt.timetuple())\n",
    "    except OverflowError:\n",
    "        timestamp = diff.total_seconds()\n",
    "    \n",
    "    return timestamp\n",
    "\n",
    "\n",
    "date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']\n",
    "\n",
    "# Replace NaN values with value of most occuring record. We can also drop all records containing NaNs, but for the time being\n",
    "# I chose first method.\n",
    "most_occuring_value = df[date_columns].apply(pd.Series.value_counts).sum(axis=1).idxmax()\n",
    "df[date_columns] = df[date_columns].fillna(most_occuring_value)\n",
    "\n",
    "df[date_columns] = df[date_columns].applymap(_convert_string_to_unix_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE EMP_TITLE VALUES OCCURING LESS THAN 10 TIMES WITH 'OTHER'\n",
    "\n",
    "df['emp_title'] = df['emp_title'].fillna('NaN')  # Replace NaN with string value\n",
    "count_dict = df['emp_title'].value_counts().to_dict()\n",
    "df['emp_title'] = df['emp_title'].apply(lambda row: row if count_dict[row] > 10 else 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ONE-HOT REPRESENTATIONS OF ALL CATEGORICAL VALUES\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "df = pd.get_dummies(df, columns=categorical_columns, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE CARE OF REMAINING NAN (IN COLUMNS THAT HAD TYPE FLOAT FROM THE BEGGINING)\n",
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 528)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET FEATURE MATRIX\n",
    "labels = df['labels'].values\n",
    "df = df.drop(columns=['labels'])\n",
    "features = df.values\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature matrix has shape (10000, 527). Let's break our dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "dataset = np.concatenate((features, labels.reshape(features.shape[0], 1)), axis=1)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Split into train and test, no validation set since specification of this task doesn't mention it.\n",
    "split = int(dataset.shape[0] * 0.3)\n",
    "train_set, test_set = dataset[split:], dataset[:split]\n",
    "\n",
    "# Split into features and labels \n",
    "X_train, Y_train = train_set[:,:-1], train_set[:, -1]\n",
    "X_test, Y_test = test_set[:,:-1], test_set[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font size=\"10\"> Models </font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training our models.\n",
    "\n",
    "For baseline method I'm going to be using logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kamil\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the scores! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy:  0.9293333333333333 \n",
      " Precision:  0.4375 \n",
      " Recall:  0.03333333333333333 \n",
      " F1:  0.061946902654867256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "predictions_log = log_reg.predict(X_test)\n",
    "print(' Accuracy: ', accuracy_score(Y_test, predictions_log), '\\n',\n",
    "      'Precision: ', precision_score(Y_test, predictions_log), '\\n',\n",
    "      'Recall: ', recall_score(Y_test, predictions_log), '\\n',\n",
    "      'F1: ', f1_score(Y_test, predictions_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores seem to be awful. High accuracy means almost nothing to us, recall is the score we should be trying to maximize since False Negatives are the biggest issue for us (detecting bad loans). As predicted we face a serious case of overfitting.\n",
    "\n",
    "We can now test other classification algorithms (k-nn, svm, xgboost, neural networks, lightGBM etc.), but first I would like to focus more on feature selection and reducing features dimentionality, since this may lead to reducing overfitting, especialy when our feature space is rather bloated right now.\n",
    "\n",
    "I'm going to use tree-based feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 528)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35000, 79)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "print(X_train.shape)\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_train_new = model.transform(X_train)\n",
    "\n",
    "X_train_new.shape               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've managed to reduce our features number, which not only will reduce overfitting, but will be handy in further iterations of data analysis! \n",
    "\n",
    "Currently I would like to get back to exploring the data and selected features, starting with the variables that were selected by tree-based method as most important. Only after that I would explore different models. Unfortunetly due to time constrictions I need to skip this step.\n",
    "\n",
    "Let's see how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy:  0.9677333333333333 \n",
      " Precision:  0.9982394366197183 \n",
      " Recall:  0.54 \n",
      " F1:  0.7008652657601978\n"
     ]
    }
   ],
   "source": [
    "predictions_clf = clf.predict(X_test)\n",
    "print(' Accuracy: ', accuracy_score(Y_test, predictions_clf), '\\n',\n",
    "      'Precision: ', precision_score(Y_test, predictions_clf), '\\n',\n",
    "      'Recall: ', recall_score(Y_test, predictions_clf), '\\n',\n",
    "      'F1: ', f1_score(Y_test, predictions_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but still recall is not high enough for the model to be useful. Higher precision is good (we're not labeling good loans as bad), but we're still not detecting bad loans good enough. \n",
    "\n",
    "Let's try out more powerful model (to be precise, ensemble of models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb = xgb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy:  0.9950666666666667 \n",
      " Precision:  0.9969450101832994 \n",
      " Recall:  0.9323809523809524 \n",
      " F1:  0.9635826771653544\n"
     ]
    }
   ],
   "source": [
    "predictions_xgb = xgb.predict(X_test)\n",
    "print(' Accuracy: ', accuracy_score(Y_test, predictions_xgb), '\\n',\n",
    "      'Precision: ', precision_score(Y_test, predictions_xgb), '\\n',\n",
    "      'Recall: ', recall_score(Y_test, predictions_xgb), '\\n',\n",
    "      'F1: ', f1_score(Y_test, predictions_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunetly that is all the time I can dedicate towards this task. If I was given more of it, I would probably approach this problem further in this way:\n",
    "\n",
    "1. As mentioned before, feature selection seem to be interesting and important task. It would require me to revisit data and see which features are important, see with which variables they correlate and which specific examples models are unable to detect.\n",
    "\n",
    "2. After further research I would attempt training ensambles of models and test them more rigorously (via k-fold validation, plotting ROC curves against precision-recall curves). I would probably try to use regularization (L1, L2, data augmentation, random noise etc.).\n",
    "\n",
    "3. I would test different algorithms (mainly Neural Networks) and attempt tuning hyperparameters via grid-search.\n",
    "\n",
    "4. Plot some graphs! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
